{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Machine Learning for Big Data\n",
    "## Lab assignment: machine learning project on Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 9, we presented three different case studies of design of machine learning for Big Data. The aim of this lab is to offer you an opportunity to put your hands-on to design/develop a Big Data solution. All the project ideas presented in this section are general descriptions and will allow you to have a preliminary idea of what the project will be, but we are expecting you to do research and determine the final shape of the project. The three case studies of Chapter 9 are example of these machine learning project. You can use them as example of depth we would be expecting you to get to.\n",
    "\n",
    "> When we teach our Big Data module, this lab assignment is part of a larger assessment, which consists of imitating how a research conference works (involving abstract submission, full submission, peer-review, final submission and presentation). We have provided a copy of our coursework for those of you interested in this type of assessment. \n",
    "\n",
    "**Important to bear in mind**: You may not have enough resources (i.e., computing nodes) to perform real Big Data. You are not expected to run your code on extremely Big Datasets which might take a very long time. However, that does not prevent us from developing and designing Big Data solutions. You simply need to use smaller sets. As a thumb rule, we wouldn't be expecting you to run anything taking longer than 1 day of execution, unless this is just of a ‘sequential’ program that you are using as a baseline. This means that you are expected to use subsets of the original datasets if they are very big. Note that creating those subsets might not be trivial in all cases, as you still want them to be representative (i.e., preserve the class distribution!). \n",
    "\n",
    "> *Use of Deep Learning*: Whilst it is okay to use Deep Learning techniques, this is usually best parallelized using GPUs which is not the goal of this book. You can certainly parallelize Deep Learning on Spark (e.g., using TensorFlow On Spark), but this is not our goal. You are however able to use deep learning, if necessary, after processing Big Data with Apache Spark, but this shouldn't be the focus of your project. \n",
    "\n",
    "Best Tips:\n",
    "- An experimental design is super important when talking about Big Data. Estimating the potential length of all the experiments you want to carry out is key to control the time. E.g., we would never try to test my code or any minor changes on a 5 million instance dataset. \n",
    "- Determine the kind of experimental validation need for your topic. E.g., selecting an appropriate performance measure or following a standard 5-fold cross validation.\n",
    "- Measure the scalability of your solution (scale-up, size-up, speedup)\n",
    "- Analyze the effects in performance according to multiple parameters. For example, varying number of maps is very important for the accuracy/error rates in local models, and runtime in all models.\n",
    "\n",
    "\n",
    "The projects have been split into two different categories: problem-based and technique-based.\n",
    "\n",
    "- **Problem-based project**: Aim to exploit some data to solve a problem using a machine learning solution/pipeline for this in Big Data. The focus should be on the problem itself, so that, the selection of techniques and designs required should be specific for the problem at hand. You can use different data sources for this type of project:\n",
    "    - Sensor data: e.g., Temperature of buildings.\n",
    "    - Social media: e.g., Twitter messages.\n",
    "    \n",
    "A good example of this could be a Kaggle competition, which usually provides lots of data. \n",
    "\n",
    "- **Technique-based project**:  Aim to enable a machine learning technique to deal with Big Data. The data is not the most important aspect for this kind of project. This may include (but it is not limited to):\n",
    "    - Data pre-processing: e.g., feature selection, dimensionality reduction, noise filtering, missing values imputation, etc.\n",
    "    - Unsupervised learning: e.g., DBSCAN, hierarchical clustering.\n",
    "    - Time series or Regression: e.g., ARIMA, Logistic regression, SVRs.\n",
    "    - Non-standard classification/regression: e.g., multi-label classification, multi-output, imbalanced classification, semi-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem-based projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of project focuses on the application of Big Data to solve a problem, rather than on developing 'new' machine learning techniques for a general purpose. This doesn't mean that you should only use existing algorithms, but you are expected to solve a problem using a Big Data solution. This might mean that you need to adapt an existing technique to tackle the problem or you investigate how to use/combine several existing methods to do this.\n",
    "\n",
    "As you will be dealing with real-world problems, you cannot expect the data to be in the final shape to perform machine learning. You will have to model that data in a particular way to be able to do this. This will also include data that is far from perfect, including missing values, noise, irrelevant features/instances or inconsistencies. It is expected that you take that into consideration when designing your solution that could for example be a pipeline of subsequent machine learning techniques to get the final output. Some of the project ideas might allow for various projects, so you need to clearly identify what you plan to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PB01: Predicting Contact Maps in Bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contact map prediction is a bioinformatics problem, and more specifically a protein structure prediction classification task. The ECBLD’14 Big Data competition provided a highly imbalanced dataset to solve this problem. We won this competition using Big Data solutions in Hadoop, which were mostly focused on Data Preprocessing. However, we didn’t explore many other classifiers, as at the time we didn't have Apache Spark and its great implementations. Also, we focused on oversampling approaches which are certainly NOT ideal for Big Data (as we generated even more data!). So, the challenge of this project is: can you beat our solution or perform similarly with a simpler/better approach?\n",
    "\n",
    "**References:**\n",
    "- [ECBLD’14 Big Data competition](http://cruncher.ico2s.org/bdcomp/)\n",
    "- [ROSEFW-RF: The winner algorithm for the ECBDL’14 big data competition: An extremely imbalanced big data bioinformatics problem](https://www.sciencedirect.com/science/article/abs/pii/S0950705115002130)\n",
    "    - [Github repository](https://github.com/triguero/ROSEFW-RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PB02: Prediction of protein subcellular localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a very relevant bioinformatics problem. Protein subcellular localization is a well-studied topic and many machine learning algorithms have been proposed to solve it. However, most of them are supervised classification approaches and rely on annotations that are often not available in this area because it requires lots of expert's knowledge and time. The aim of this project is to investigate the use of (Big Data) semi-supervised techniques to tackle the problem. Whilst Deep Learning might be appealing for this and could be used as comparison algorithms, the project should investigate other (Big Data) alternatives. You should identify the kind of semi-supervised approaches you would like to investigate/develop.\n",
    "\n",
    "**References:**\n",
    "- [DeepLoc: prediction of protein subcellular localization using deep learning](https://pubmed.ncbi.nlm.nih.gov/29036616/)\n",
    "- [Human Protein Atlas, HPA, Dataset](https://www.kaggle.com/lidasong/hpv-ieee-ml-sjtu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PB03: Hot Spots Identification/Monitoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hot spot is defined as a region of high likelihood of occurrence of a particular event. To identify hot spots, location data for those events is required, which is typically collected by telematics devices. These sensors are constantly gathering information, generating very large volumes of data. In previous research, we developed a Big Data stream solution to detect hot spots in real-time, which was applied to a transportation example. The goal of this project is to investigate alternative approaches and alternative databases which will require different processing and modifications in the algorithm. In particular, you could focus on crime hotspots which are areas on a map that have high crime intensity. \n",
    "\n",
    "**References:**\n",
    "- You can get lots of location data for Crime here: [UK Police Data repository](https://data.police.uk/data/)\n",
    "- [PAS3-HSID: a Dynamic Bio-Inspired Approach for Real-Time Hot Spot Identification in Data Streams](https://link.springer.com/article/10.1007%2Fs12559-019-09638-y)\n",
    "    - [Github repository](https://github.com/beccatickle/PAS-HSID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PB04: Predicting and understanding the Crypto Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting stock market or the crypto market is probably one of the most appealing time-series prediction problem. Many have attempted various time-series prediction algorithms, and algorithms such as Facebook Prophet or LSTMs seem to be the best performing methods. As such, this is not necessarily Big Data. For this to be Big Data, we need to add more variety and maybe even deal with the velocity. The main challenge with the stock market, in general, is the BIG influence of social media (producing sudden changes) and news going around which make people to panic sell (decreasing the price) or buy like crazy (increasing the price). One funny thing about bitcoin is how the entire crypto market tends to fall apart like a house of cards, but sometimes some coins remain stronger (which ones? And why?). In this project, you should investigate the use of Big Data solutions to either predict or understand the crypto market. There are many different things you could do, e.g., classifying time series, clustering them, and use that to later make predictions. But again, the focus shouldn't only be in predicting more accurately, but on the innovative use of Big Data solutions.\n",
    "\n",
    "**References:**\n",
    "- [Cryptocurrency Historical Prices](https://www.kaggle.com/sudalairajkumar/cryptocurrencypricehistory)\n",
    "- [Bitcoin Historical Data](https://www.kaggle.com/mczielinski/bitcoin-historical-data)\n",
    "- [Huge Stock Market Dataset](https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PB05: A Classification Model for Sentiment analysis of Tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a huge topic, and you could focus on any particular application as soon as you can gather enough data.  Here we suggest you aim to classify twitter text messages according to hot hashtags for US election 2020, which was proposed recently in Kaggle. To tackle this problem, your solution should be able to quickly extract the text data features to highlight the hot keywords which are re-tweeted more than once, and apply various machine learning algorithms to understand if there is any correlation between what's going on twitter and the actual results in the elections. Alternatively, you could use other databases, such as the climate sentiment in twitter dataset. \n",
    "\n",
    "**References:**\n",
    "- [Analysis of Political Sentiment Orientations on Twitte](https://www.sciencedirect.com/science/article/pii/S1877050920306669)\n",
    "- [US Election 2020 Tweets](https://www.kaggle.com/manchunhui/us-election-2020-tweets)\n",
    "- [Climate sentiment in twitter](https://www.kaggle.com/joseguzman/climate-sentiment-in-twitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PB06: A Big Data Classification Solution for Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main topics in banking, finances or insurance companies is Fraud detection.  There are many different machine learning algorithms to classify transactions as fraudulent vs non-fraudulent, but some of these systems make some mistakes that can be embarrassing for the customers, if they, for example, get the credit card declined. This is typically modeled as an imbalanced classification problem. Our colleagues from the IEEE Computational Intelligence society organized a Challenge on this in 2019 and provided a real-world dataset available on Kaggle. This project should investigate alternative Big Data approaches to detect fraud detection.\n",
    "\n",
    "**References:**\n",
    "- [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection)\n",
    "- [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)  (another potential dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique-based projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, these projects are quite loose. We want you to decide what you want to do within each specific topic. Again, you will find project descriptions that offer more than one project, and you should identify what you want to do exactly.\n",
    "\n",
    "You should consider only publicly available data. Some really Big Datasets we know of: [HIGGS](https://archive.ics.uci.edu/ml/datasets/HIGGS), [SUSY](https://archive.ics.uci.edu/ml/datasets/SUSY), [Bitcoin](https://archive.ics.uci.edu/ml/datasets/BitcoinHeistRansomwareAddressDataset), [PokerHand](https://archive.ics.uci.edu/ml/datasets/Poker+Hand), [ECBDL14](http://cruncher.ico2s.org/bdcomp/). For smaller datasets that can be useful in classification, regression, or time-series: [KEEL](https://sci2s.ugr.es/keel/datasets.php), [UCI](https://archive.ics.uci.edu/ml/index.php), [UCR](http://www.cs.ucr.edu/~eamonn/time_series_data/), and [Mulan](http://mulan.sourceforge.net/datasets.html). \n",
    "\n",
    "For an incomplete list of data mining methods in each category you could browse through the selection of methods provided in KEEL for standard data mining: [here](https://sci2s.ugr.es/keel/algorithms.php).  You should conduct a thorough literature review, but we thought that was a good starting point for many of the topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TB01: Instance Reduction in Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2015, we developed a local-based MapReduce solution for Instance Reduction in Big Data (Classification). The aim of these methods is to represent the original training data sets as a reduced (manageable) number of instances. Our model was implemented in Apache Hadoop and followed a simple divide-and-conquer approach to allow ANY existing instance reduction technique to be applied in Big Data. After that, in 2018, we developed another simple yet competitive global prototype generation algorithm. In this project, you are asked to implement instance reduction algorithms in Apache Spark. This could consist of replicating what we designed in our previous works, or you could (aim higher and) look for other instance selection/generation techniques that can be designed as a global big data solutions. You can follow a similar experimental methodology as in the papers below, although we would recommend starting with smaller datasets.\n",
    "\n",
    "**References:**\n",
    "- [MRPR: A MapReduce solution for prototype reduction in big data classification](https://www.sciencedirect.com/science/article/pii/S0925231214013009)\n",
    "    - [Github repository](https://github.com/triguero/MRPR)\n",
    "- [CHI-PG: A fast prototype generation algorithm for Big Data classification problems](https://www.sciencedirect.com/science/article/pii/S0925231218300894)\n",
    "    - [Github repository](https://github.com/melkilin/CHI-PG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TB02: Feature selection in Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a very hot topic in machine learning and data mining. On the one hand, it allows to reduce the dimensionality of a problem which is needed to learn appropriate models and reduce the volume of the data. On the other hand, it allows us to identify relevant features and even establish a ranking of features (very interesting in bioinformatics). There are many approaches to select/rank features: filter, wrapper, and embedded feature selection (e.g., implicit selection performed by a RandomForest). In Big Data, there are some solutions to deal with this, which are mostly focused on filter approaches and univariate models (because they are faster). In this project, you should investigate different feature selection approaches in the context of Big Data with a high number of features. You should identify the kind of approaches you will focus on and if your target is to reduce the dimensionality or determine a ranking of features.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [Evolutionary Feature Selection for Big Data Classification: A MapReduce Approach](https://www.hindawi.com/journals/mpe/2015/246139/)\n",
    "    - [Github repository](https://github.com/triguero/MR-EFS)\n",
    "- [An Information Theory-Based Feature Selection Framework for Big Data Under Apache Spark](https://ieeexplore.ieee.org/document/7970198)\n",
    "    - [Spark package](https://spark-packages.org/package/sramirez/spark-infotheoretic-feature-selection)\n",
    "- [A Preliminary Study of the Feasibility of Global Evolutionary Feature Selection for Big Datasets under Apache Spark](https://ieeexplore.ieee.org/document/8477878)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TB03: Pre-processing techniques for Imbalanced learning in Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the funniest things in the context of Big Data is that in many cases we still suffer from data scarcity. This usually happens in real problems like in medicine where we have very skewed datasets. For example, we may easily gather lots data from healthy patients but to predict rare diseases we might not have sufficient information. Machine learning and data mining technique usually struggle to cope with this imbalanced situation. One way to deal with this is by using pre-processing techniques, which aim to find a better balance of the input data prior to any learning. We have worked a lot on under-sampling and over-sampling approaches for imbalance datasets in Big Data, but we usually used traditional methods such as: Random undersampling vs. evolutionary undersampling, or Random oversampling vs. SMOTE. In this project, you should investigate alternative approaches to pre-process imbalanced classification datasets (e.g., SMOTE-ENN, RUSBoost, Repeated Edited Nearest Neighbours) or you could attempt to deal with multi-class imbalance problems.\n",
    "\n",
    "\n",
    "**References:**\n",
    "- [Imbalanced Classification for Big Data](https://link.springer.com/chapter/10.1007/978-3-319-98074-4_13)\n",
    "- [Multi-class imbalanced big data classification on Spark](https://www.sciencedirect.com/science/article/abs/pii/S0950705120307279)\n",
    "- [A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches](https://ieeexplore.ieee.org/document/5978225?arnumber=5978225)\n",
    "- [A first attempt on global evolutionary undersampling for imbalanced big data](https://ieeexplore.ieee.org/abstract/document/7969553)\n",
    "- [Evolutionary undersampling for extremely imbalanced big data classification under apache spark](https://ieeexplore.ieee.org/abstract/document/7743853)\n",
    "    - [Github repository](https://github.com/triguero/EUS-BigData)\n",
    "- [Imbalance learn library](https://github.com/scikit-learn-contrib/imbalanced-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TB04: Advanced Clustering in Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering techniques are very important to understand our data. In the current MLlib API of Apache spark we find classical approaches such as k-means or GMMs. However, we do have many more clustering algorithms such as DBSCAN, BIRCH, spectral clustering, or biclustering which could be very useful in Big Data. Scikit-learn has some good algorithms, but they do not scale-up to Big Data. In this project, you should pick some representative clustering algorithm(s) and design a Big Data solution for it. You should identify the algorithms you are interested. Note that there are some Spark-based implementations of some of them (like DBSCAN), which you should find (as part of your literature review) and determine if they are good or not. \n",
    "\n",
    "**References:**\n",
    "- [Clustering in scikit-learn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster)\n",
    "- [MR-DBSCAN: a scalable MapReduce-based DBSCAN algorithm for heavily skewed data](https://link.springer.com/article/10.1007/s11704-013-3158-3)\n",
    "- [DBSCAN on Spark](https://github.com/irvingc/dbscan-on-spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TB05: Time-series classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of time-series, we are not only interested in predicting the future, but also in classifying them. This is for example very useful for speech recognition or activity recognition (based on sensors). Apart from Deep Learning-based approaches, the k-Nearest Neighbours (kNN) algorithm has extensively been used for this, but instead of using the Euclidean distance, it uses something called: Dynamic Time Warping (DTW), which allow us to compute distances between Time Series. In this project, you are asked to investigate the classification of time series in the Big Data context. Could you enable the classic k-NN with DTW to scale up to Big Datasets with plenty of very lengthy time-series? \n",
    "\n",
    "**References:**\n",
    "- [kNN-IS: An Iterative Spark-based design of the k-Nearest Neighbors classifier for big data](https://www.sciencedirect.com/science/article/pii/S0950705116301757)\n",
    "    - [Github repository](https://github.com/JMailloH/kNN_IS)\n",
    "- [k-Nearest Neighbors & Dynamic Time Warping](https://github.com/markdregan/K-Nearest-Neighbors-with-Dynamic-Time-Warping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TB06: (Fuzzy) Rule-based classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy Rule-Based Classification Systems (FRBCSs) are used in multiple applications because their output is usually a set of human-readable rules which might be relatively interpretable, which is a big challenge in Big Data if we may produce too many rules. In the paper below, we designed a state-of-the-art fuzzy rule-based system for Big Data classification that was focused on a very traditional fuzzy rule-based algorithm: Chi's algorithm. The aim of this project is to investigate the suitability of other (fuzzy or not) rule-based classifiers in the context of Big Data Classification. Note that rules could even be derived from a Decision Tree! >You could also aim to implement this in Spark, since this was originally developed in Hadoop.\n",
    "\n",
    "**References:**\n",
    "- [CHI-BD: A fuzzy rule-based classification system for Big Data classification problems](https://www.sciencedirect.com/science/article/abs/pii/S0165011417302774?via%3Dihub)\n",
    "    - [Github repository](https://github.com/melkilin/CHI-BD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TB07: Multi-target Prediction for Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-target prediction, an instance has to be classified along multiple target variables at the same time, where each target represents categories or numerical values. There are several strategies to tackle multi-target prediction problems: the local strategy learns a separate model for each target variable independently, while the global strategy learns a single model for all target variables together. You could target this project with different perspectives depending on the predictive tasks: multi-target regression, multi-target classification, or even hierarchical multi-label classification. You should choose one of them and target it in the context of Big Data. Funnily enough, in Big Data, we rarely find classifiers or regressors capable of dealing with multiple outputs. If you choose this project, you should identify which subproblem and algorithm(s) you will consider (e.g., Extending the Decision Tree of Apache Spark to multi-label classification, using a global learning strategy). \n",
    "\n",
    "**References:**\n",
    "- [Tree ensembles for predicting structured outputs](https://www.sciencedirect.com/science/article/abs/pii/S003132031200430X)\n",
    "- [MULAN Dataset repository](http://mulan.sourceforge.net/datasets.html)\n",
    "- [Beyond global and local multi-target learning](https://www.sciencedirect.com/science/article/pii/S0020025521008227)\n",
    "    - [Github repository](https://github.com/rcerri/Clus-Hyper-Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TB08: Semi-supervised Random Forest or Support Vector Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may remember from Chapter 9, obtaining labeled data can be a very expensive and laborious task, while unlabeled data can be abundant. The availability of labeled data can seriously limit the performance of supervised learning methods. Rather than following self-labeling techniques, some classical methods have been adjusted to use unlabeled data during the training process. In this project you could choose Decision Trees/RandomForest or Support Vector Machines to tackle big amounts of unlabeled data. As we did in Chapter 9, you should use standard classification datasets and modify them to simulate different ratios of labeled data vs unlabeled data. If you choose this project, you should specifically identify the algorithm (e.g., RandomForest or Support Vector Machines) you would consider. \n",
    "\n",
    "**References:**\n",
    "\n",
    "- [Semi-supervised classification trees](https://link.springer.com/article/10.1007%2Fs10844-017-0457-4)\n",
    "- [Semi-Supervised Random Forests](https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Documents/lrs/pubs/leistner_iccv_09.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
