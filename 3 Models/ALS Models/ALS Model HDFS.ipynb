{"cells":[{"cell_type":"markdown","id":"d9f73813","metadata":{},"source":["# 0. Setup notebook"]},{"cell_type":"code","execution_count":1,"id":"d7c8c589","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/conda/miniconda3/lib/python3.11/site-packages (3.5.1)\n","Requirement already satisfied: filelock in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (15.0.2)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.12.2)\n","Requirement already satisfied: aiohttp in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (3.11.18)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Found existing installation: pyarrow 15.0.2\n","Uninstalling pyarrow-15.0.2:\n","  Successfully uninstalled pyarrow-15.0.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Collecting pyarrow==15.0.2\n","  Using cached pyarrow-15.0.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy<2,>=1.16.6 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pyarrow==15.0.2) (1.26.4)\n","Using cached pyarrow-15.0.2-cp311-cp311-manylinux_2_28_x86_64.whl (38.3 MB)\n","Installing collected packages: pyarrow\n","Successfully installed pyarrow-15.0.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install datasets\n","%pip uninstall -y pyarrow\n","%pip install pyarrow==15.0.2"]},{"cell_type":"code","execution_count":2,"id":"caf8dde2","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from datasets import load_dataset, load_dataset_builder\n","import subprocess"]},{"cell_type":"code","execution_count":4,"id":"ba67a95d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 22:04:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["spark = (SparkSession.builder.master(\"yarn\").appName(\"ALS-tune\")\n","         .config(\"spark.driver.memory\",\"8g\")\n","         .config(\"spark.executor.memory\",\"8g\")\n","         .config(\"spark.driver.maxResultSize\",\"2g\")\n","         .getOrCreate())"]},{"cell_type":"markdown","id":"2a008d86","metadata":{},"source":["----"]},{"cell_type":"markdown","id":"7d2c1d0c","metadata":{},"source":["# 1. Loading and preparing dataset"]},{"cell_type":"code","execution_count":5,"id":"3484d1cf","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80735b6eb20b498490dd95e589fecd31","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/30.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8aa8f42c11f1451fb5455e7d156cdbc9","version_major":2,"version_minor":0},"text/plain":["Amazon-Reviews-2023.py:   0%|          | 0.00/39.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73968450eee8456aa3fc6b18a4e012a3","version_major":2,"version_minor":0},"text/plain":["All_Beauty.jsonl:   0%|          | 0.00/327M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0eb4a09410764f7f8b43a83eb495211b","version_major":2,"version_minor":0},"text/plain":["Generating full split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'rating': 5.0, 'title': 'Such a lovely scent but not overpowering.', 'text': \"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\", 'images': [], 'asin': 'B00YQ6X8EO', 'parent_asin': 'B00YQ6X8EO', 'user_id': 'AGKHLEW2SOWHNMFQIJGBECAF7INQ', 'timestamp': 1588687728923, 'helpful_vote': 0, 'verified_purchase': True}\n"]}],"source":["from datasets import load_dataset\n","\n","dataset_user = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)\n","print(dataset_user[\"full\"][0])"]},{"cell_type":"code","execution_count":6,"id":"104238ed","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca869062b3d849b6b51bc500d4ff48f4","version_major":2,"version_minor":0},"text/plain":["meta_All_Beauty.jsonl:   0%|          | 0.00/213M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25913e633acd45f6b764d9ab1b3df780","version_major":2,"version_minor":0},"text/plain":["Generating full split:   0%|          | 0/112590 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'main_category': 'All Beauty', 'title': 'Howard LC0008 Leather Conditioner, 8-Ounce (4-Pack)', 'average_rating': 4.8, 'rating_number': 10, 'features': [], 'description': [], 'price': 'None', 'images': {'hi_res': [None, 'https://m.media-amazon.com/images/I/71i77AuI9xL._SL1500_.jpg'], 'large': ['https://m.media-amazon.com/images/I/41qfjSfqNyL.jpg', 'https://m.media-amazon.com/images/I/41w2yznfuZL.jpg'], 'thumb': ['https://m.media-amazon.com/images/I/41qfjSfqNyL._SS40_.jpg', 'https://m.media-amazon.com/images/I/41w2yznfuZL._SS40_.jpg'], 'variant': ['MAIN', 'PT01']}, 'videos': {'title': [], 'url': [], 'user_id': []}, 'store': 'Howard Products', 'categories': [], 'details': '{\"Package Dimensions\": \"7.1 x 5.5 x 3 inches; 2.38 Pounds\", \"UPC\": \"617390882781\"}', 'parent_asin': 'B01CUPMQZE', 'bought_together': None, 'subtitle': None, 'author': None}\n"]}],"source":["dataset_item = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_All_Beauty\", split=\"full\")\n","print(dataset_item[0])"]},{"cell_type":"code","execution_count":7,"id":"dd99824b","metadata":{},"outputs":[],"source":["user_reviews = dataset_user\n","item_metadata = dataset_item"]},{"cell_type":"code","execution_count":9,"id":"ce4e9b83","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"efe7efcb2be84be99db898f77817acd2","version_major":2,"version_minor":0},"text/plain":["Creating json from Arrow format:   0%|          | 0/702 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb51d9114f1c49feb07e847dbbd3b27f","version_major":2,"version_minor":0},"text/plain":["Creating json from Arrow format:   0%|          | 0/113 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["206804886"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# For user_reviews, which is a DatasetDict\n","user_reviews[\"full\"].to_json(\"user_reviews.jsonl\")\n","\n","# For item_metadata, which is a Dataset\n","item_metadata.to_json(\"item_metadata.jsonl\")"]},{"cell_type":"code","execution_count":10,"id":"1521da36","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2 items\r\n","-rw-r--r--   2 root hadoop  206804886 2025-05-05 22:05 /user/hdfs/raw_meta_All_Beauty.jsonl\r\n","-rw-r--r--   2 root hadoop  314275749 2025-05-05 22:05 /user/hdfs/raw_review_All_Beauty.jsonl\r\n"]}],"source":["# push the data onto HDFS and check the folder \n","!hadoop fs -put user_reviews.jsonl /user/hdfs/raw_review_All_Beauty.jsonl\n","!hadoop fs -put item_metadata.jsonl /user/hdfs/raw_meta_All_Beauty.jsonl\n","!hadoop fs -ls /user/hdfs"]},{"cell_type":"code","execution_count":11,"id":"58f1b8ee","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{\"rating\":5.0,\"title\":\"Such a lovely scent but not overpowering.\",\"text\":\"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\",\"images\":[],\"asin\":\"B00YQ6X8EO\",\"parent_asin\":\"B00YQ6X8EO\",\"user_id\":\"AGKHLEW2SOWHNMFQIJGBECAF7INQ\",\"timestamp\":1588687728923,\"helpful_vote\":0,\"verified_purchase\":true}\r\n","{\"rating\":4.0,\"title\":\"Works great but smells a little weird.\",\"text\":\"This product does what I need it to do, I just wish it was odorless or had a soft coconut smell. Having my head smell like an orange coffee is offputting. (granted, I did know the smell was described but I was hoping it would be light)\",\"images\":[],\"asin\":\"B081TJ8YS3\",\"parent_asin\":\"B081TJ8YS3\",\"user_id\":\"AGKHLEW2SOWHNMFQIJGBECAF7INQ\",\"timestamp\":1588615855070,\"helpful_vote\":1,\"verified_purchase\":true}\r\n","{\"rating\":5.0,\"title\":\"Yes!\",\"text\":\"Smells good, feels great!\",\"images\":[],\"asin\":\"B07PNNCSP9\",\"parent_asin\":\"B097R46CSY\",\"user_id\":\"AE74DYR3QUGVPZJ3P7RFWBGIX7XQ\",\"timestamp\":1589665266052,\"helpful_vote\":2,\"verified_purchase\":true}\r\n","{\"rating\":1.0,\"title\":\"Synthetic feeling\",\"text\":\"Felt synthetic\",\"images\":[],\"asin\":\"B09JS339BZ\",\"parent_asin\":\"B09JS339BZ\",\"user_id\":\"AFQLNQNQYFWQZPJQZS6V3NZU4QBQ\",\"timestamp\":1643393630220,\"helpful_vote\":0,\"verified_purchase\":true}\r\n","{\"rating\":5.0,\"title\":\"A+\",\"text\":\"Love it\",\"images\":[],\"asin\":\"B08BZ63GMJ\",\"parent_asin\":\"B08BZ63GMJ\",\"user_id\":\"AFQLNQNQYFWQZPJQZS6V3NZU4QBQ\",\"timestamp\":1609322563534,\"helpful_vote\":0,\"verified_purchase\":true}\r\n","cat: Unable to write to output stream.\r\n"]}],"source":["!hadoop fs -cat /user/hdfs/raw_review_All_Beauty.jsonl | head -n 5\n"]},{"cell_type":"code","execution_count":12,"id":"50067479","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{\"main_category\":\"All Beauty\",\"title\":\"Howard LC0008 Leather Conditioner, 8-Ounce (4-Pack)\",\"average_rating\":4.8,\"rating_number\":10,\"features\":[],\"description\":[],\"price\":\"None\",\"images\":{\"hi_res\":[null,\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/71i77AuI9xL._SL1500_.jpg\"],\"large\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41qfjSfqNyL.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41w2yznfuZL.jpg\"],\"thumb\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41qfjSfqNyL._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41w2yznfuZL._SS40_.jpg\"],\"variant\":[\"MAIN\",\"PT01\"]},\"videos\":{\"title\":[],\"url\":[],\"user_id\":[]},\"store\":\"Howard Products\",\"categories\":[],\"details\":\"{\\\"Package Dimensions\\\": \\\"7.1 x 5.5 x 3 inches; 2.38 Pounds\\\", \\\"UPC\\\": \\\"617390882781\\\"}\",\"parent_asin\":\"B01CUPMQZE\",\"bought_together\":null,\"subtitle\":null,\"author\":null}\r\n","{\"main_category\":\"All Beauty\",\"title\":\"Yes to Tomatoes Detoxifying Charcoal Cleanser (Pack of 2) with Charcoal Powder, Tomato Fruit Extract, and Gingko Biloba Leaf Extract, 5 fl. oz.\",\"average_rating\":4.5,\"rating_number\":3,\"features\":[],\"description\":[],\"price\":\"None\",\"images\":{\"hi_res\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/71g1lP0pMbL._SL1500_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/81OqvR94isL._SL1500_.jpg\"],\"large\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41b+11d5igL.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41j2ocUzCtL.jpg\"],\"thumb\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41b+11d5igL._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41j2ocUzCtL._SS40_.jpg\"],\"variant\":[\"MAIN\",\"PT01\"]},\"videos\":{\"title\":[],\"url\":[],\"user_id\":[]},\"store\":\"Yes To\",\"categories\":[],\"details\":\"{\\\"Item Form\\\": \\\"Powder\\\", \\\"Skin Type\\\": \\\"Acne Prone\\\", \\\"Brand\\\": \\\"Yes To\\\", \\\"Age Range (Description)\\\": \\\"Adult\\\", \\\"Unit Count\\\": \\\"10 Fl Oz\\\", \\\"Is Discontinued By Manufacturer\\\": \\\"No\\\", \\\"Item model number\\\": \\\"SG_B076WQZGPM_US\\\", \\\"UPC\\\": \\\"653801351125\\\", \\\"Manufacturer\\\": \\\"Yes to Tomatoes\\\"}\",\"parent_asin\":\"B076WQZGPM\",\"bought_together\":null,\"subtitle\":null,\"author\":null}\r\n","{\"main_category\":\"All Beauty\",\"title\":\"Eye Patch Black Adult with Tie Band (6 Per Pack)\",\"average_rating\":4.4,\"rating_number\":26,\"features\":[],\"description\":[],\"price\":\"None\",\"images\":{\"hi_res\":[null,null],\"large\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/31bz+uqzWCL.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/31bz+uqzWCL.jpg\"],\"thumb\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/31bz+uqzWCL._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/31bz+uqzWCL._SS40_.jpg\"],\"variant\":[\"MAIN\",\"PT01\"]},\"videos\":{\"title\":[],\"url\":[],\"user_id\":[]},\"store\":\"Levine Health Products\",\"categories\":[],\"details\":\"{\\\"Manufacturer\\\": \\\"Levine Health Products\\\"}\",\"parent_asin\":\"B000B658RI\",\"bought_together\":null,\"subtitle\":null,\"author\":null}\r\n","{\"main_category\":\"All Beauty\",\"title\":\"Tattoo Eyebrow Stickers, Waterproof Eyebrow, 4D Imitation Eyebrow Tattoos, 4D Hair-like Authentic Eyebrows Waterproof Long Lasting for Woman & Man Makeup Tool\",\"average_rating\":3.1,\"rating_number\":102,\"features\":[],\"description\":[],\"price\":\"None\",\"images\":{\"hi_res\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/71GJhXQGvyL._SL1500_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/61NS1lONhzL._SL1001_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/61lcwXtw3ZL._SL1001_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/61G-iZeX-LL._SL1001_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/618BBBsAQrL._SL1001_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/61iomcZjbAL._SL1001_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/61m-71vCbCL._SL1001_.jpg\"],\"large\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/515iwxdKS1L.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/51ALSVnGDML.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/51GvPall-ML.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/51JFEUql-KL.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/414rIT4vNAL.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41JVLUdtYaL.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/51Vxn6nVrVL.jpg\"],\"thumb\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/515iwxdKS1L._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/51ALSVnGDML._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/51GvPall-ML._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/51JFEUql-KL._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/414rIT4vNAL._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/41JVLUdtYaL._SS40_.jpg\",\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/51Vxn6nVrVL._SS40_.jpg\"],\"variant\":[\"MAIN\",\"PT01\",\"PT02\",\"PT03\",\"PT04\",\"PT05\",\"PT06\"]},\"videos\":{\"title\":[],\"url\":[],\"user_id\":[]},\"store\":\"Cherioll\",\"categories\":[],\"details\":\"{\\\"Brand\\\": \\\"Cherioll\\\", \\\"Item Form\\\": \\\"Powder\\\", \\\"Finish Type\\\": \\\"Natural\\\", \\\"Product Benefits\\\": \\\"Long Lasting\\\", \\\"Skin Type\\\": \\\"All\\\", \\\"Package Dimensions\\\": \\\"8.43 x 5.91 x 0.87 inches; 8.78 Ounces\\\", \\\"Item model number\\\": \\\"eyebrow sticker001\\\"}\",\"parent_asin\":\"B088FKY3VD\",\"bought_together\":null,\"subtitle\":null,\"author\":null}\r\n","{\"main_category\":\"All Beauty\",\"title\":\"Precision Plunger Bars for Cartridge Grips \\u2013 93mm \\u2013 Bag of 10 Plungers\",\"average_rating\":4.3,\"rating_number\":7,\"features\":[\"Material: 304 Stainless Steel; Brass tip\",\"Lengths Available: 88mm, 93mm, 98mm\",\"Accepts cartridge needles with vice style tattoo machines\",\"Works perfectly with Precision Disposable Soft Cartridge Grips\",\"Price per one bag of 10 plungers\"],\"description\":[\"The Precision Plunger Bars are designed to work seamlessly with the\\u00a0Precision Disposable 1. 25\\\" Contoured Soft Cartridge Grips\\u00a0and the\\u00a0Precision Disposable 1\\\" Textured Soft Cartridge Grips\\u00a0to drive cartridge needles with vice style or standard tattoo machine setups. These plunger bars are manufactured from 304 Stainless Steel and feature a brass tip. The plungers are sold in a bag of ten in your choice of 88mm, 93mm, or 98mm length.\"],\"price\":\"None\",\"images\":{\"hi_res\":[null],\"large\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/31TgqAZ8kQL.jpg\"],\"thumb\":[\"https:\\/\\/m.media-amazon.com\\/images\\/I\\/31TgqAZ8kQL._SS40_.jpg\"],\"variant\":[\"MAIN\"]},\"videos\":{\"title\":[],\"url\":[],\"user_id\":[]},\"store\":\"Precision\",\"categories\":[],\"details\":\"{\\\"UPC\\\": \\\"644287689178\\\"}\",\"parent_asin\":\"B07NGFDN6G\",\"bought_together\":null,\"subtitle\":null,\"author\":null}\r\n","cat: Unable to write to output stream.\r\n"]}],"source":["!hadoop fs -cat /user/hdfs/raw_meta_All_Beauty.jsonl | head -n 5"]},{"cell_type":"markdown","id":"496a9455","metadata":{},"source":["---"]},{"cell_type":"markdown","id":"eb07ddde","metadata":{},"source":["# 2. ALS Model"]},{"cell_type":"markdown","id":"2d9b7f61","metadata":{},"source":["## 2.1 Loading up data into ALS model"]},{"cell_type":"code","execution_count":2,"id":"db0c49a3","metadata":{},"outputs":[],"source":["from pyspark.sql.types import *\n","from pyspark.sql.functions import col, explode, split, regexp_replace\n","from pyspark.ml.recommendation import ALS\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","id":"c2ee2f4b","metadata":{},"source":["First of all we are loading the previously saved user and item review data from the HDFS in form of json file data into our dataframes. In order to get a first feel of how the data looks we are printing the schema as well as two first example rows of the respective dataframes."]},{"cell_type":"code","execution_count":3,"id":"48a3052f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["User Review Schema:\n","root\n"," |-- asin: string (nullable = true)\n"," |-- helpful_vote: long (nullable = true)\n"," |-- images: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- attachment_type: string (nullable = true)\n"," |    |    |-- large_image_url: string (nullable = true)\n"," |    |    |-- medium_image_url: string (nullable = true)\n"," |    |    |-- small_image_url: string (nullable = true)\n"," |-- parent_asin: string (nullable = true)\n"," |-- rating: double (nullable = true)\n"," |-- text: string (nullable = true)\n"," |-- timestamp: long (nullable = true)\n"," |-- title: string (nullable = true)\n"," |-- user_id: string (nullable = true)\n"," |-- verified_purchase: boolean (nullable = true)\n","\n","+----------+------------+------+-----------+------+--------------------+-------------+--------------------+--------------------+-----------------+\n","|      asin|helpful_vote|images|parent_asin|rating|                text|    timestamp|               title|             user_id|verified_purchase|\n","+----------+------------+------+-----------+------+--------------------+-------------+--------------------+--------------------+-----------------+\n","|B00YQ6X8EO|           0|    []| B00YQ6X8EO|   5.0|This spray is rea...|1588687728923|Such a lovely sce...|AGKHLEW2SOWHNMFQI...|             true|\n","|B081TJ8YS3|           1|    []| B081TJ8YS3|   4.0|This product does...|1588615855070|Works great but s...|AGKHLEW2SOWHNMFQI...|             true|\n","+----------+------------+------+-----------+------+--------------------+-------------+--------------------+--------------------+-----------------+\n","only showing top 2 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["\n","Item Metadata Schema:\n","root\n"," |-- author: string (nullable = true)\n"," |-- average_rating: double (nullable = true)\n"," |-- bought_together: string (nullable = true)\n"," |-- categories: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- description: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- details: string (nullable = true)\n"," |-- features: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- images: struct (nullable = true)\n"," |    |-- hi_res: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n"," |    |-- large: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n"," |    |-- thumb: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n"," |    |-- variant: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n"," |-- main_category: string (nullable = true)\n"," |-- parent_asin: string (nullable = true)\n"," |-- price: string (nullable = true)\n"," |-- rating_number: long (nullable = true)\n"," |-- store: string (nullable = true)\n"," |-- subtitle: string (nullable = true)\n"," |-- title: string (nullable = true)\n"," |-- videos: struct (nullable = true)\n"," |    |-- title: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n"," |    |-- url: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n"," |    |-- user_id: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n","\n","+------+--------------+---------------+----------+-----------+--------------------+--------+--------------------+-------------+-----------+-----+-------------+---------------+--------+--------------------+------------+\n","|author|average_rating|bought_together|categories|description|             details|features|              images|main_category|parent_asin|price|rating_number|          store|subtitle|               title|      videos|\n","+------+--------------+---------------+----------+-----------+--------------------+--------+--------------------+-------------+-----------+-----+-------------+---------------+--------+--------------------+------------+\n","|  NULL|           4.8|           NULL|        []|         []|{\"Package Dimensi...|      []|{[NULL, https://m...|   All Beauty| B01CUPMQZE| None|           10|Howard Products|    NULL|Howard LC0008 Lea...|{[], [], []}|\n","+------+--------------+---------------+----------+-----------+--------------------+--------+--------------------+-------------+-----------+-----+-------------+---------------+--------+--------------------+------------+\n","only showing top 1 row\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r\n","[Stage 3:>                                                          (0 + 1) / 1]\r\n","\r\n","                                                                                \r"]}],"source":["# Loading user review dataframe + printing schema\n","df_user_review = spark.read.json(\"hdfs:///user/hdfs/raw_review_All_Beauty.jsonl\")\n","print(\"User Review Schema:\")\n","df_user_review.printSchema()\n","df_user_review.show(2)\n","\n","# Loading item metadata dataframe + printing schema\n","df_item_metadata = spark.read.json(\"hdfs:///user/hdfs/raw_meta_All_Beauty.jsonl\")\n","print(\"\\nItem Metadata Schema:\")\n","df_item_metadata.printSchema()\n","df_item_metadata.show(2)"]},{"cell_type":"markdown","id":"3fd14ddc","metadata":{},"source":["After loading the data we are creating a specific dataframe where we are going to store specific user_ids and their according asin and ratings for the products. Then we need to select the necessary columns for the ALS model which are the user_id, item_id and ultimately the rating which is helpiing us to understand by how much a user is likely to buy an item according to similar users. The rating goes from 0-10. Our main dataframe will be the so called als_df which is represented by the A matrix in the report. Moreover we are printing some statistics to get a better overview of what data we have at hand inside matrix A."]},{"cell_type":"code","execution_count":6,"id":"d15f5fc1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 22:37:07 WARN DAGScheduler: Broadcasting large task binary with size 27.2 MiB\n","25/05/05 22:37:18 WARN DAGScheduler: Broadcasting large task binary with size 27.2 MiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["\n","Prepared data sample:\n"]},{"name":"stderr","output_type":"stream","text":["25/05/05 22:37:22 WARN DAGScheduler: Broadcasting large task binary with size 34.4 MiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+--------+------+\n","| userId|  itemId|rating|\n","+-------+--------+------+\n","|33934.0|   686.0|   5.0|\n","|33934.0|  9899.0|   4.0|\n","|71462.0| 14563.0|   5.0|\n","|26005.0|112872.0|   1.0|\n","|26005.0| 43503.0|   5.0|\n","+-------+--------+------+\n","only showing top 5 rows\n","\n","\n","Null values count:\n","+-------+\n","|summary|\n","+-------+\n","|  count|\n","|   mean|\n","| stddev|\n","|    min|\n","|    25%|\n","|    50%|\n","|    75%|\n","|    max|\n","+-------+\n","\n","\n","Ratings statistics:\n"]},{"name":"stderr","output_type":"stream","text":["25/05/05 22:37:26 WARN DAGScheduler: Broadcasting large task binary with size 30.9 MiB\n","[Stage 11:======================================>                   (2 + 1) / 3]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|summary|            rating|\n","+-------+------------------+\n","|  count|            701528|\n","|   mean|3.9602453501499584|\n","| stddev|1.4944515968822232|\n","|    min|               1.0|\n","|    max|               5.0|\n","+-------+------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r\n","                                                                                \r"]}],"source":["# 1. DATA PREPARATION\n","# Select relevant columns for the recommendation system\n","ratings_df = df_user_review.select(\"user_id\", \"asin\", \"rating\")\n","\n","# Convert the user_id (string) to a numeric index for ALS\n","from pyspark.sql.functions import monotonically_increasing_id\n","from pyspark.ml.feature import StringIndexer\n","\n","# Create numeric user IDs\n","user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"userId\", handleInvalid=\"skip\")\n","user_indexed_model = user_indexer.fit(ratings_df)\n","ratings_df = user_indexed_model.transform(ratings_df)\n","\n","# Create numeric item IDs\n","item_indexer = StringIndexer(inputCol=\"asin\", outputCol=\"itemId\", handleInvalid=\"skip\")\n","item_indexed_model = item_indexer.fit(ratings_df)\n","ratings_df = item_indexed_model.transform(ratings_df)\n","\n","# Select necessary columns for ALS model\n","als_df = ratings_df.select(\"userId\", \"itemId\", \"rating\")\n","\n","# Verify data preparation\n","print(\"\\nPrepared data sample:\")\n","als_df.show(5)\n","\n","# Check for any null values\n","print(\"\\nNull values count:\")\n","als_df.select([col(c).isNull().alias(c) for c in als_df.columns]).summary().show()\n","\n","# Basic statistics for ratings\n","print(\"\\nRatings statistics:\")\n","als_df.describe(\"rating\").show()"]},{"cell_type":"markdown","id":"8d10f088","metadata":{},"source":["## 2.2 Splitting data"]},{"cell_type":"markdown","id":"b8ec1990","metadata":{},"source":["In order to train our ALS model and consequently evaluate it as well, we are splitting the A matrix into a training and testing part with a split of 80/20, where 80% of the data will be used for training the ALS model and 20% will not be shown to the model so that the weights for the X and Y matrix are not influenced by testing data. Therefore within the testing scenario the RMSE, root mean square error, will be assessed as the performance metric."]},{"cell_type":"code","execution_count":7,"id":"ab9c09cc","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 22:37:34 WARN DAGScheduler: Broadcasting large task binary with size 34.4 MiB\n","25/05/05 22:37:42 WARN DAGScheduler: Broadcasting large task binary with size 34.4 MiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Training data count: 561128\n"]},{"name":"stderr","output_type":"stream","text":["25/05/05 22:37:46 WARN DAGScheduler: Broadcasting large task binary with size 34.4 MiB\n","25/05/05 22:37:53 WARN DAGScheduler: Broadcasting large task binary with size 34.4 MiB\n","[Stage 19:>                                                         (0 + 3) / 3]\r"]},{"name":"stdout","output_type":"stream","text":["Test data count: 140400\n"]},{"name":"stderr","output_type":"stream","text":["\r\n","                                                                                \r"]}],"source":["# 2. SPLIT DATA FOR TRAINING AND TESTING\n","# Split data into training and test sets\n","(training_data, test_data) = als_df.randomSplit([0.8, 0.2], seed=42)\n","\n","# Cache the datasets for improved performance\n","training_data.cache()\n","test_data.cache()\n","\n","print(f\"Training data count: {training_data.count()}\")\n","print(f\"Test data count: {test_data.count()}\")"]},{"cell_type":"markdown","id":"24c93325","metadata":{},"source":["Quickly we will check how man RDD partitions have been created to handle this dataset."]},{"cell_type":"code","execution_count":8,"id":"8bb4157e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of partitions:  3\n","Number of workers: 2\n"]}],"source":["# Getting the number of current RDD partitions\n","print('Number of partitions: ', training_data.rdd.getNumPartitions())\n","\n","# Requesting the number of worker nodes assigned to master node\n","print('Number of workers:', sc._conf.get('spark.executor.instances'))"]},{"cell_type":"markdown","id":"983b94ed","metadata":{},"source":["Due to multiple issues like crashing kernels (due to the size of the data) in GCP when training the ALS model, other solutions needed to be tested. We decided to increase the number of partitions which is creating a batched environment for the ALS training so the VMs are not training the whole dataset but smaller parts of it when running the RDDs into the ALS."]},{"cell_type":"code","execution_count":9,"id":"fc9f5e45","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 22:38:04 WARN DAGScheduler: Broadcasting large task binary with size 34.4 MiB\n","25/05/05 22:38:09 WARN DAGScheduler: Broadcasting large task binary with size 20.1 MiB\n","25/05/05 22:38:48 WARN DAGScheduler: Broadcasting large task binary with size 20.1 MiB\n","25/05/05 22:39:23 WARN DAGScheduler: Broadcasting large task binary with size 34.4 MiB\n","25/05/05 22:39:26 WARN DAGScheduler: Broadcasting large task binary with size 20.1 MiB\n","25/05/05 22:39:57 WARN DAGScheduler: Broadcasting large task binary with size 20.1 MiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["New number of partitions for training data:  200\n","New number of partitions for testing data:  200\n"]}],"source":["# Choosing a partition count suitable for large dataset size\n","training_data = training_data.repartition(200).persist()  \n","test_data = test_data.repartition(200).persist()\n","\n","# Materializing the cache\n","training_data.count()\n","test_data.count()\n","print('New number of partitions for training data: ', training_data.rdd.getNumPartitions())\n","print('New number of partitions for testing data: ', test_data.rdd.getNumPartitions())"]},{"cell_type":"markdown","id":"daca7585","metadata":{},"source":["## 2.3 Training ALS"]},{"cell_type":"markdown","id":"0da50701","metadata":{},"source":["Ultimately we train the ALS using the library from pyspark, and we predefine arbitarily the hyperparameters. We chose a quite low amount of latent features and a low amount of maximum iterations due to the aforementioned high likelihood of the kernel crashing while training. In order ot keep the size as low as possible, we reduced the dimensionality of the matrices by reducing the latent features to 3 instead holding them at 5 or 10, since the matrices X and Y are having their dimension set by k = latent features."]},{"cell_type":"code","execution_count":11,"id":"73ab1775","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 22:42:48 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:42:50 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:43:31 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:44:11 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:44:17 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:44:49 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:44:53 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:44:57 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:00 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:04 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:08 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:11 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:14 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:17 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:20 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:24 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:26 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:30 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","                                                                                \r"]}],"source":["# 3. TRAIN ALS MODEL\n","# Initialize ALS model\n","als = ALS(\n","    rank=3,                      # Number of latent factors\n","    maxIter=5,                   # Maximum number of iterations\n","    regParam=0.1,                # Regularization parameter\n","    userCol=\"userId\",            # User column name\n","    itemCol=\"itemId\",            # Item column name\n","    ratingCol=\"rating\",          # Rating column name\n","    coldStartStrategy=\"drop\"     # Strategy to handle cold start problem    \n",").setCheckpointInterval(2)       # cut lineage every 2 iterations\n","\n","# Train the model\n","model = als.fit(training_data)"]},{"cell_type":"markdown","id":"e939261a","metadata":{},"source":["## 2.4 Evaluating ALS"]},{"cell_type":"markdown","id":"cf1affb8","metadata":{},"source":["After our first ALS model has been trained we are evaluating the ALS model with our testing set using the performance metric root mean squared error, which is common when it comes to continous optimization methods."]},{"cell_type":"code","execution_count":12,"id":"a1afac28","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/05/05 22:45:41 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:42 WARN DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n","25/05/05 22:45:43 WARN DAGScheduler: Broadcasting large task binary with size 20.1 MiB\n","25/05/05 22:46:26 WARN DAGScheduler: Broadcasting large task binary with size 20.3 MiB\n","[Stage 127:==========================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["\n","Root Mean Squared Error (RMSE): 5.0500559582618205\n"]},{"name":"stderr","output_type":"stream","text":["\r\n","                                                                                \r"]}],"source":["# 4. EVALUATE THE MODEL\n","# Make predictions on test data\n","predictions = model.transform(test_data)\n","\n","# Evaluate model using RMSE (Root Mean Squared Error)\n","evaluator = RegressionEvaluator(\n","    metricName=\"rmse\", \n","    labelCol=\"rating\", \n","    predictionCol=\"prediction\"\n",")\n","\n","rmse = evaluator.evaluate(predictions)\n","print(f\"\\nRoot Mean Squared Error (RMSE): {rmse}\")"]},{"cell_type":"markdown","id":"f32ed623","metadata":{},"source":["Here we are saving the ALS model for HDFS, however this can be only used when staying within the same cluster and not spinning up a new cluster to tetry training of the notebook and ALS."]},{"cell_type":"code","execution_count":null,"id":"1f101020","metadata":{},"outputs":[],"source":["# SAVING THE FIRST MODEL AND USER/ITEM MAPPINGS\n","\n","# Define location where model will be saved\n","hdfs_path = \"hdfs:///st446-cluster-w09-m\"\n","\n","# Save the first model due to multiple kernel crashes occuring during running/training\n","model.save(hdfs_path + \"amazon_first_als_model\")\n","\n","# Save user and item mappings for future use\n","ratings_df.select(\"user_id\", \"userId\").distinct().write.parquet(hdfs_path + \"user_mapping\")\n","ratings_df.select(\"asin\", \"itemId\").distinct().write.parquet(hdfs_path + \"item_mapping\")\n","\n","print(\"\\nModel and mappings saved successfully\")"]},{"cell_type":"code","execution_count":null,"id":"a3f54257","metadata":{},"outputs":[],"source":["from pyspark.ml.recommendation import ALSModel\n","\n","hdfs_model_path = \"hdfs:///user/st446-cluster-w09-m/amazon_first_als_model\"\n","loaded_model = ALSModel.load(hdfs_model_path)\n","\n","user_map = spark.read.parquet(\"hdfs:///user/{YOUR_LINUX_USERNAME}/user_mapping\")\n","item_map = spark.read.parquet(\"hdfs:///user/{YOUR_LINUX_USERNAME}/item_mapping\")\n"]},{"cell_type":"markdown","id":"6e735d75","metadata":{},"source":["From here on we will focus on the main notebook \"ALS Model Local.ipynb\" as it is the notebook which was able to run and train the ALS reasonably without too many kernel crashes."]},{"cell_type":"markdown","id":"3f9b84a5","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}
